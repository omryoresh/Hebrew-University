---
title: "Final Project - Machine Learning"
author: "Omry Yoresh"
date: "June 2023"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction:

Construction accidents pose a significant threat to worker safety, productivity, and the economy at large. Simultaneously, air pollution has emerged as a global concern, impacting various aspects of human life. Understanding the relationship between pollution and construction accidents is crucial for improving workplace safety and mitigating the adverse effects of pollution. This project aims to extend the main results of the paper "Heads Up: Does Pollution Cause Construction Accidents?" by Victor Lavy, Genia Rachkovsky, and myself, within the framework of the course "Machine Learning for Economists" at The Hebrew University of Jerusalem.

The original study examined the potential impact of Nitrogen Dioxide ($NO_2$) on construction site accidents. Building upon this research, the project leverages machine learning techniques and methodologies learned in the course to further explore and validate the findings. Machine learning offers unique advantages in uncovering complex data patterns, extracting meaningful insights, and improving predictive accuracy.

To accomplish this, the project adopts a two-fold approach. Firstly, a double lasso methodology will be employed to validate the controls selected for analyzing the effect of $NO_2$. Given the availability of multiple pollutants and weather variables, this analysis will identify the most influential factors associated with construction accidents. It will also potentially address concerns of co-determination of pollution in both time and space, enhancing the robustness of the original findings.

Moreover, the project goes beyond replication and validation by incorporating a Ridge prediction model. This model will aim to assess the likelihood of a construction accident at a given construction site. A comparative analysis will be conducted to evaluate the accuracy of the model using same-day data versus lagged data. This analysis holds potential policy implications, as it can inform decisions regarding safety protocols. Understanding the predictive power of different time frames is crucial for effectively managing construction site safety and labor allocation.

The project is organized as follows: Chapter 2 focuses on data replication to ensure consistency with the original study. Chapter 3 employs the double lasso method to validate the chosen pollution variables and explore potential regressors. Chapter 4 compares and develops prediction models to assess the likelihood of construction accidents using both same-day and lagged data. Finally, Chapter 5 summarizes the key findings, discusses their implications, and provides recommendations for policymakers.


# 2. Data Replication:
### 2.1 Data Preparation:
The dataset used in this paper combines data from three primary sources: the Israeli Ministry of Economy and Industry, which provided information on construction sites' locations, activity dates, and construction accidents between 2017 and 2019; the Israeli Ministry of Environmental Protection, which provided measurements of air pollution and weather during the same period; and Kav LaOved, a nonprofit organization focused on workers' rights, which supplemented the construction site accident data.

### 2.1.1 Construction Sites:
Initially, the Ministry of Economy and Industry provided a sample of 25,571 active construction sites in Israel from 2017 to 2019. By applying geo-coding techniques, the sites' addresses were matched to their respective coordinates. Each active day for each site was then assigned an observation, resulting in a final sample of 24,614 sites and 10,016,000 observations.

### 2.1.2 Accidents:
The accident sample provided by the Ministry of Economy and Industry included 1,316 accidents within the specified period. However, the accidents provided by Kav LaOved did not have site IDs that matched the ministry's data. Therefore, the accidents were matched to the sites based on their addresses, resulting in an additional 31 accidents. By merging the dataset of the site's active days and the accidents, a total of 1,164 accidents were recorded per 10,016,000 working days in construction sites.

### 2.1.3 Environmental Data:
Air pollution and weather data were obtained from the Israeli Ministry of Environmental Protection. The data included 8-hour average measurements of various pollutants such as Nitrogen Dioxide ($NO_2$), wind strength and direction, temperature, humidity, and other pollutants from 173 monitoring stations across Israel during the sample period. Each active day in a construction site was assigned the nearest reading for each variable, with 7,199 construction sites having at least one monitoring station within a 1 km distance.

### 2.2 Summary Statistics:
Summary statistics were computed for the pollutants and weather variables. The following table (Table 1) presents the average rates and standard errors of the various pollutants and weather variables measured:

Load libraries:
```{r}
set.seed(42)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse,     # for data wrangling 
  tidymodels,    # for ml regressions
  broom,         # convert results to tidy objects
  fastDummies,   # for turning categorical variables into sets of dummies
  fixest,        # for twfe estimation and se clustering
  hdm,           # for double lasso
  gridExtra,     # for graphs
  haven,         # uploading dta files
  rpart,         # for random forest
  rattle,        # for data mining
  magrittr,      # for piping operations
  caret,         # for machine learning modeling
  DALEX,         # for model explanations
  rpart.plot,    # for plotting decision trees
  RColorBrewer,  # for color palettes
  ada,           # for adaptive boosting
  doParallel,    # for parallel computing
  ggplot2,       # for data visualization
  kableExtra,    # for creating tables
  rsample,       # for data splitting
  GGally,        # for pairwise plots
  broom,         # convert results to tidy objects
  glmnet,        # for elastic net regularization
  yardstick      # for model evaluation metrics
  )
```

Loading raw pollution data:
```{r, message=FALSE}
load("/Users/omry/Documents/GitHub/Heads-Up--Does-Air-Pollution-Cause-Workplace-Accidents/Data/Process/Pollution.Rda")
```

I first create the summary statistics of the pollutants and weather variables (Table 1).
```{r, message=FALSE, warning=FALSE}
pollution_data_tidy <- pollution %>% 
  pivot_longer(cols = c("no2_08", "no2_16", "no2_24", "pm25_08",
                        "pm25_16", "pm25_24", "temp_08", "temp_16",
                        "temp_24", "wind_08", "wind_16", "wind_24",
                        "humidity_08", "humidity_16", "humidity_24",
                        "so2_08", "so2_16", "so2_24", "o3_08", "o3_16",
                        "o3_24", "pm10_08", "pm10_16", "pm10_24", "co_08",
                        "co_16", "co_24"), 
               names_to = c("Pollutant", "Hour Measured"), 
               names_sep = "_")

pollution_summary <- pollution_data_tidy %>% 
  group_by(Pollutant, `Hour Measured`) %>% 
  summarise(Units = ifelse(grepl("no2", Pollutant), "ppb",
                    ifelse(grepl("pm", Pollutant), "µg/m3",
                    ifelse(grepl("temp", Pollutant), "Celsius",
                    ifelse(grepl("wind", Pollutant), "m/sec",
                    ifelse(grepl("humidity", Pollutant), "%",
                    ifelse(grepl("so2", Pollutant), "ppb",
                    ifelse(grepl("o3", Pollutant), "ppb",
                    ifelse(grepl("pm10", Pollutant), "µg/m3",
                    ifelse(grepl("co", Pollutant), "ppm", NA
                           ))))))))), 
            Monitors = n_distinct(mon_id, na.rm = TRUE),
            Obs = sum(!is.na(value)),
            `Average Rate` = round(mean(value, na.rm = TRUE), 1),
            `Standard Error` = round(sd(value, na.rm = TRUE), 1),
            .groups = "drop_last") %>% 
  arrange(Pollutant, `Hour Measured`) %>%
  unique()
```

```{r, echo = FALSE}
knitr::kable(pollution_summary, align = "lcccccc")
```

```{r, include=FALSE}
rm(pollution_data_tidy, pollution_summary, pollution)
```

Next I plot the distribution of the construction accidents by days of the week and months:
```{r, message=FALSE, warning=FALSE}
load("/Users/omry/Documents/GitHub/Heads-Up--Does-Air-Pollution-Cause-Workplace-Accidents/Data/Final/Pollution and Accidents - Final.Rda")

data$dayofweek <- factor(data$dayofweek, levels = c("Sunday", "Monday",
                                                    "Tuesday", "Wednesday",
                                                    "Thursday", "Friday", "Saturday"))

# Converting day of the week to a factor with levels ordered
accidents_dayweek <- aggregate(acc ~ dayofweek, data, FUN = sum) 

# Creating a data-frame with counts of accidents by day of week
p1 <- ggplot(accidents_dayweek, aes(x = dayofweek, y = acc)) + 
  geom_bar(stat = "identity") + 
  geom_text(aes(label = acc), vjust = -0.5, size = 3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = NULL, y = NULL) +
  theme_classic() +
  scale_fill_manual(values = rainbow(7)) 

# Creating a bar chart for day of week:
accidents_month <- aggregate(acc ~ month, data, FUN = sum)
accidents_month$month <- month.abb[accidents_month$month] 

# Creating a data-frame with counts of accidents by month
p2 <- ggplot(accidents_month, aes(x = month, y = acc)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  geom_text(aes(label = acc), vjust = -0.5, size = 3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = NULL, y = NULL) +
  theme_classic() +
  scale_x_discrete(labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", 
                                              "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) +
  scale_fill_manual(values = rainbow(3))

p2 <- p2 + theme(plot.margin = margin(0.5, 0, 0, 0, "in"))
p <- grid.arrange(p1, p2, ncol = 1, nrow = 2, heights = c(4, 4))

rm(data, accidents_dayweek, accidents_month, p, p1, p2)
```

# 2.3 Econometric Framework and Main Analysis:
### 2.3.1 Econometric Framework:
The main analysis in this paper employs a linear probability fixed effects model to estimate the effect of $\beta$ in the following equation:
$$
  Y_{st} = \alpha + \beta NO_2+f(Temp_{st}, Wind_{st}, Hum_{st})+S_s+DMY_t+\epsilon_{st} ,
$$
In this equation, $s$ represents the construction site index, while $t$ represents the day index. The outcome variable $Y_{st}$ is a binary indicator that determines whether an accident occurred at construction site $s$ on day $t$. The nitrogen dioxide level ($NO_2$) measured in parts per billion ($ppb$) at the monitoring station closest to the construction site between 8 am and 4 pm is included as a variable of interest. The equation also includes construction site fixed effects denoted as $S_s$ and time fixed effects denoted as $DMY_t$ (day of the week, month, and year). The weather variables, $f(Temp_{st}, Wind_{st}, Hum_{st})$, represent temperature, wind speed, and humidity levels, respectively, along with the squared weather measurements obtained from the closest monitoring station. The idiosyncratic error term is denoted as $\epsilon_{st}$, and standard errors are clustered based on the construction site.

The purpose of this model is to address several challenges in establishing a causal relationship between pollution levels and the likelihood of a construction accident. The first challenge is the potential endogeneity of pollution levels due to their correlation with environmental factors such as temperature, wind, and humidity. To mitigate this issue, the model includes flexible controls for weather variables in the regression function.

The second challenge involves endogeneity associated with specific permanent attributes of each construction site. If pollution levels vary among contractors with different safety standards or if less experienced workers are more likely to work in regions with higher pollution levels, endogeneity concerns arise. To account for this, the model incorporates construction site fixed effects, allowing for an examination of within-construction site variations in pollution levels and accident probabilities.

Measurement error is another potential concern in the literature on air pollution impacts. When monitoring station density or measurement frequency is low, the risk of biased results due to measurement error increases. To address this issue, the analysis takes advantage of a large number of monitoring stations spread across the country. The observations are limited to construction sites with a monitoring station within a 1 km radius. Additionally, pollution levels during working hours (8 am to 4 pm) are utilized since readings are available for three different intervals per day.

A major concern is that high pollution levels may originate from the construction site itself on busy or specific days when the accident likelihood is higher. To address this concern, the analysis employs two instrumental variable approaches. Firstly, pollution levels at the nearest monitoring station (within a 1 km radius) are instrumented using average pollution levels from stations within a 5–10 km radius. This assumes that any pollution generated at the construction site would have minimal impact on monitoring stations located more than 5 km away.

Secondly, lagged pollution levels measured at the closest monitoring station during the preceding night are used as an instrument. This instrument assumes that pollution levels measured the night before only influence the probability of a construction accident during working hours on the same day through pollution levels measured during those working hours. It is also assumed that pollution generated by the site itself cannot affect pollution levels measured the previous night when the site is largely inactive.

Formally, the instrumental variable model is represented by the following equations:

First Stage:
$$
NO_{2st} = \alpha+\lambda NO_{2s,t-0.5}+f(Temp_{st}, Wind_{st}, Hum_{st})+S_s+DMY_t+\nu_{st}
$$

$$
NO_{2st} = \alpha+\lambda NO_{2g_{5-10km},st}+f(Temp_{st}, Wind_{st}, Hum_{st})+S_s+DMY_t+\mu_{st}
$$

Second Stage:
$$
  Y_{st} = \alpha + \beta Pred(NO_2)+f(Temp_{st}, Wind_{st}, Hum_{st})+S_s+DMY_t+\epsilon_{st}
$$
The paper also explores a non-linear model. Considering that international organizations and governments establish standards and guidelines concerning exposure to elevated air pollution levels, the non-linear model investigates the impact of high pollution levels as follows:

$$
  Y_{st} = \alpha + \beta ModerateNO_2+\gamma HighNO_{2st}+f(Temp_{st}, Wind_{st}, Hum_{st})+S_s+DMY_t+\epsilon_{st},
$$
In this model, the continuous measure of air pollution in the previous models is replaced with dummy variables for clean, moderately polluted, and highly polluted days. Moderately polluted days are defined as days when $NO_2$ levels are higher than $53$ $ppb$, roughly corresponding to the $95^{th}$ percentile in the sample, which is considered moderate pollution by the EPA. Highly polluted days are defined as days when $NO_2$ levels exceed $100$ $ppb$ according to EPA standards, roughly corresponding to the $99^{th}$ percentile in the sample.

### 2.3.2 Replicating Main Analysis Table:
Before replicating the main table results (Table 2), the next chunk creates variables which are important for the analysis and the extension.
```{r}
data <- read_dta("/Users/omry/Documents/GitHub/Heads-Up--Does-Air-Pollution-Cause-Workplace-Accidents/Data/Final/Pollution and Accidents - Final for R.dta")

data <- data %>%
  mutate(
    no2_16_crit   = ifelse(no2_16<53, 1, ifelse(no2_16>=53 & no2_16<100, 2, 3)),
    temp_16_sq    = (temp_16*temp_16),
    wind_16_sq    = (wind_16*wind_16),
    humidity_16_sq= (humidity_16*humidity_16),
    temp_lag2     = lag(temp_24, 1),
    temp_lag3     = lag(temp_16, 1),
    temp_lag4     = lag(temp_08, 1),
    humidity_lag2 = lag(humidity_24, 1),
    humidity_lag3 = lag(humidity_16, 1),
    humidity_lag4 = lag(humidity_08, 1),
    wind_lag2     = lag(wind_24, 1),
    wind_lag3     = lag(wind_16, 1),
    wind_lag4     = lag(wind_08, 1),
    no2_lag2      = lag(no2_24, 1),
    no2_lag3      = lag(no2_16, 1),
    no2_lag4      = lag(no2_08, 1),
    no2_lag1      = no2_08,
    co_lag1       = co_08,
    co_lag2       = lag(co_24, 1),
    co_lag3       = lag(co_16, 1),
    co_lag4       = lag(co_08, 1),
    pm25_lag1     = pm25_08,
    pm25_lag2     = lag(pm25_24, 1),
    pm25_lag3     = lag(pm25_16, 1),
    pm25_lag4     = lag(pm25_08, 1),
    pm10_lag1     = pm10_08,
    pm10_lag2     = lag(pm10_24, 1),
    pm10_lag3     = lag(pm10_16, 1),
    pm10_lag4     = lag(pm10_08, 1),
    o3_lag1       = o3_08,
    o3_lag2       = lag(o3_24, 1),
    o3_lag3       = lag(o3_16, 1),
    o3_lag4       = lag(o3_08, 1),
    so2_lag1      = so2_08,
    so2_lag2      = lag(so2_24, 1),
    so2_lag3      = lag(so2_16, 2),
    so2_lag4      = lag(so2_08, 1)
  )

dict <- setFixest_dict(c(acc = "Accident", no2_16 = "Nitrogen Dioxide (ppb) 8am-4pm",
                                           pm25_16 = "Particulate Matter 2.5 (ug/m3) 8am-4pm",
                                           temp_16 = "Temperature (c)",
                                           temp_16_sq = "Temperature^2 (c)",
                                           wind_16 = "Wind (m/s)",
                                           wind_16_sq = "Wind^2 (m/s)",
                                           humidity_16 = "Humidity (%)",
                                           humidity_16_sq = "Humidity^2 (%)",
                                           year = "Year",
                                           month = "Month",
                                           dayofweek = "Day of the Week",
                                           site_id = "Construction Site",
                                           "factor(no2_16_crit)3" = "Unhealthy",
                                           "factor(no2_16_crit)2" = "Moderate"))
# Setting up a fixest dictionary for variables
```

Now, replicating the main analysis table:
```{r, message=FALSE, warning=FALSE}
# Regression models
# Table 2
# Model 1
est.t2.1 <- feols(acc ~ no2_16,
                  cluster = ~site_id,
                  data %>% filter(no2_16_d<1))

# Model 2
est.t2.2 <- feols(acc ~ no2_16 + wind_16 + wind_16_sq + temp_16 + temp_16_sq + 
                    humidity_16 + humidity_16_sq | factor(month) + factor(year) +
                    factor(dayofweek) + factor(site_id),
                    cluster = ~site_id,
                    data %>% filter(no2_16_d<1))

# Model 3
est.t2.3 <- feols(acc ~ wind_16 + wind_16_sq + temp_16 + temp_16_sq + 
                    humidity_16 + humidity_16_sq | factor(month) + factor(year) +
                    factor(dayofweek) + factor(site_id) |
                    no2_16 ~ no2_16_5_10,
                    cluster = ~site_id,
                    data %>% filter(no2_16_d<1))

est.t2.3reduced <- feols(acc ~ no2_16_5_10 + wind_16 + wind_16_sq + temp_16 + 
                   temp_16_sq + humidity_16 + humidity_16_sq | 
                   factor(month) + factor(year) + factor(dayofweek) + 
                   factor(site_id),
                   cluster = ~site_id,
                   data %>% filter(no2_16_d<1))

# Model 4
est.t2.4 = feols(acc ~ wind_16 + wind_16_sq + temp_16 + temp_16_sq + 
                   humidity_16 + humidity_16_sq | factor(month) +
                   factor(year) + factor(dayofweek) + factor(site_id) |
                   no2_16 ~ no2_08,
                   cluster = ~site_id,
                   data %>% filter(no2_16_d<1 & no2_08_d<1))

est.t2.4reduced = feols(acc ~ no2_08 + wind_16 + wind_16_sq + temp_16 + temp_16_sq + 
                    humidity_16 + humidity_16_sq | factor(month) + factor(year) + 
                    factor(dayofweek) + factor(site_id),
                    cluster = ~site_id,
                    data %>% filter(no2_16_d<1 & no2_08_d<1))

# Model 5
est.t2.5 = feols(acc ~ factor(no2_16_crit) + wind_16 + wind_16_sq + temp_16 + 
                   temp_16_sq + humidity_16 + humidity_16_sq  |
                   factor(month) + factor(year) + factor(dayofweek) + 
                   factor(site_id),
                   cluster = ~site_id,
                   data %>% filter(no2_16_d<1))
```

```{r, include=FALSE}
t2 <- etable(est.t2.1, est.t2.2, est.t2.3, est.t2.4, est.t2.5,
             keep =c("%no2_16", "%no2_08", "%no2_16_5_10",
                     "%factor(no2_16_crit)2", "%factor(no2_16_crit)3"),
             order = c("NO2 p-value"),
             coefstat = "se",
             title = c("Table 2: Effect of Nitrogen Dioxide (NO~2~) on the Probability of a Construction Work Accident"),
             se.below = TRUE,
             fitstat = c("n"),
             signif.code =  c("***"= 0.01 , "**"= 0.05 , "*" = 0.10 ),
             tex = FALSE,
             digits = 5,
             digits.stats = 5,
             headers = list("OLS Without Controls" = 1, "OLS With Controls" = 1,
                            "5-10 Km Instrument" = 1, "Previous Night Instrument" = 1,
                            "OLS Non-Linear" = 1),
             extralines = list("Mean Dependent" = c(round(as.numeric(fitstat(
                                                      est.t2.1, type = "my")), digits = 5),
                                                    round(as.numeric(fitstat(
                                                      est.t2.2, type = "my")), digits = 5),                                                      round(as.numeric(fitstat(
                                                      est.t2.3, type = "my")), digits = 5),
                                                    round(as.numeric(fitstat(
                                                      est.t2.4, type = "my")), digits = 5),
                                                    round(as.numeric(fitstat(
                                                      est.t2.1, type = "my")), digits = 5)),
                               "Kliebergen-Paap Wald F-statistic"=c("",
                                                                    "",
                                                    format(fitstat(est.t2.3, "ivwald",
                                                      se = "clus")[["ivwald1::no2_16"]][["stat"]], big.mark = ","),
                                                    format(fitstat(est.t2.4, "ivwald", 
                                                      se = "clus")[["ivwald1::no2_16"]][["stat"]], big.mark = ","),
                                                                    ""),
                               "Clusters" = c("",
                                              format(est.t2.2[["fixef_sizes"]][["factor(site_id)"]], big.mark = ","),
                                              format(est.t2.3[["fixef_sizes"]][["factor(site_id)"]], big.mark = ","),
                                              format(est.t2.4[["fixef_sizes"]][["factor(site_id)"]], big.mark = ","),
                                              format(est.t2.5[["fixef_sizes"]][["factor(site_id)"]], big.mark = ",")
                                              )))
```

```{r, echo = FALSE}
knitr::kable(t2)
```

In this table, we can see regression results of the above equations.The dependent variable, in all columns, is the probability of an accident occurring at the construction site. The coefficient stated belongs to the independent variable, which is the rate of $NO_2$ between 8 am and 16 pm. The first column is the basic regression without controls. The second includes all controls stated. The third column is the results of the first instrumental variable analysis, where the instrument is the simple average of the $NO_2$ rates in the $5-10$ km radius from each construction site between 8 am and 16 pm. The forth column is the analysis with the second instrument, which is the rate between midnight and 8 am in the closest monitor with a $NO_2$ reading within a $1$ km from the site. The fifth column specifies the result of the non-linear model, where the levels are the $NO_2$ AQI moderate and unhealthy for sensitive group rates ($53$ and $100$ $ppb$, respectively). All standard errors are robust, adjusted for clusters by sites, and appear in parentheses. 

The coefficients obtained for the $NO_2$ variable in the regression models indicate a statistically significant effect on the probability of workplace accidents. Although the coefficient values may seem small, they still hold important implications. The small coefficient suggests that for a 10-unit increase in the NO2 rate between 8 am and 4 pm, the probability of a workplace accident increases by 20-25% percentage, consider the baseline or average probability of an accident in the sample.

```{r}
mean(data$acc)
```

# 3. ML Extension -- Double Lasso:

The double lasso procedure was performed to select relevant variables for the analysis. The pool from which the lasso selects variables include current time pollutants and weather variables, as well as lagged measures of those variables.

The first step I take is to demean the variables, in the construction site level, which in practice means  subtracting the site-specific means from the original values. This process ensures that the site fixed effects are considered, and thus shifts the lasso procedure to concentrate on the within-site variation.

```{r Double Lasso, message=FALSE}
# Demeaning the values, taking into account the site fixed effects:
data_dm <- demean(
  X = data %>%
    select(
      acc,
      no2_16,
      co_16,
      o3_16,
      pm25_16,
      pm10_16,
      so2_16,
      temp_16,
      temp_16_sq,
      wind_16,
      wind_16_sq,
      humidity_16,
      humidity_16_sq,
      temp_16_sq,   
      wind_16_sq,
      humidity_16_sq,
      temp_lag2,
      temp_lag3,
      temp_lag4,
      humidity_lag2,
      humidity_lag3,
      humidity_lag4,
      wind_lag2,
      wind_lag3,
      wind_lag4,
      no2_lag1,
      no2_lag2,
      no2_lag3,
      no2_lag4,
      co_lag1 ,
      co_lag2 ,
      co_lag3 ,
      co_lag4 ,
      pm25_lag1,
      pm25_lag2,
      pm25_lag3,
      pm25_lag4,
      o3_lag1 ,
      o3_lag2 ,
      o3_lag3 ,
      o3_lag4 ,
      so2_lag1,
      so2_lag2,
      so2_lag3,
      so2_lag4,      
      year,
      month,
      dayofweek
    ),
  f = data %>%
    select(
      site_id
      )
  )

# Double Lasso procedure: 
y <- data_dm$acc
d <- data_dm$no2_16
x <- data_dm %>% 
  select(-acc, -no2_16) %>%
  as.matrix()
```

Once the variables are demeaned, the double lasso procedure is performed. The lasso is applied with various environmental variables and their lagged values included in the selection process. This approach is particularly interesting as it aims to identify the transient effects of one pollutant while accounting also for their co-determinants.

```{r}
partialling_out <- rlassoEffect(x, y, d, method = "partialling out")
summary(partialling_out)

partialling_out[["selection.index"]] # Selected Variables

double_selection <- rlassoEffect(x, y, d, method = "double selection")
summary(double_selection)

double_selection[["selection.index"]] # Selected Variables

rm(data_dm)
```

Comparing the results with the original estimation, we can see that the effect size is slightly smaller in both estimations. However, there is an increase in statistical significance in the partialling out method and a decrease in the double selection. Notably, the temperature base level and squared humidity variables were not included in the regression this time. On the other hand, all the time fixed effects were chosen, as expected, due to the seasonality in accident rates and pollution levels throughout the year, and weekday.

The selected variables, including current time pollutants and lagged versions, indicate that the relevance and importance of these pollutants extend beyond their current values. This finding aligns with the literature discussing the co-determination of different variables both with each other and across time and space.

It is important to note that the double lasso procedure creates a competition among pollutants to be included in the regression. Traditional econometrics considers this as a complementary analysis, and in some instances avoids it (horse race). The similarity in results provides some validation and supports the overall robustness of the reported findings.

In summary, the double lasso procedure in this case compliments the analysis by selecting relevant controls that were not initially considered. The results reinforce the importance of pollutants, both current and lagged, in explaining the variations in accident rates. The selection process adds another perspective to traditional econometric approaches while providing validation for the original results.


### 4. ML Extension --  Prediction Models:

I continue by exploring the applications of prediction methods from machine learning to predict construction accidents based on environmental variables. I begin by evenly splitting the data into training and testing sets, with equal representation of accidents and non-accidents in both sets. I exclude site and year fixed effects from the model as my goal is to predict accidents based on environmental factors. However,I choose to include month and day of the week dummies to account for seasonality.

The methodology used in this analysis is a Ridge regularized regression, which applies an L2 penalty to the variables. First, I preprocess the data by creating dummy variables for the "dayofweek" and "month" variables. This allows us to represent these categorical variables in a suitable format for my analysis.
```{r, message=FALSE, warning=FALSE}
data <- data %>%
    dummy_cols(select_columns = c("dayofweek", "month"))

split <- data %>%
  initial_split(prop = 0.5, strata = acc)

train <- training(split)
test <- testing(split)
```

Next, I divide the data into different time intervals: real time, morning time, the night before, and the workday before. For each time interval, I separate the dependent variable (accident) and the covariates:

```{r, message=FALSE, warning=FALSE}
train_real_time <- train %>%
    select(
      acc, no2_16, co_16, o3_16,
      pm25_16, pm10_16, so2_16,
      temp_16, wind_16, humidity_16,
      starts_with("month_"),
      starts_with("dayofweek_")
  )

test_real_time <- test %>%
    select(
      acc, no2_16, co_16, o3_16,
      pm25_16, pm10_16, so2_16,
      temp_16, wind_16, humidity_16,
      starts_with("month_"),
      starts_with("dayofweek_")
  )

train_morning_time <- train %>%
    select(
      acc, no2_08, co_08, o3_08,
      pm25_08, pm10_08, so2_08,
      temp_08, wind_08, humidity_08, 
      starts_with("month_"),
      starts_with("dayofweek_")
  )

test_morning_time <- test %>%
    select(
      acc, no2_08, co_08, o3_08,
      pm25_08, pm10_08, so2_08,
      temp_08, wind_08, humidity_08, 
      starts_with("month_"),
      starts_with("dayofweek_")
  )

train_night_before <- train %>%
    select(
      acc, no2_lag2, co_lag2, o3_lag2,
      pm25_lag2, pm10_lag2, so2_lag2,
      temp_lag2, wind_lag2, humidity_lag2, 
      starts_with("month_"),
      starts_with("dayofweek_")
  )

test_night_before <- test %>%
    select(
      acc, no2_lag2, co_lag2, o3_lag2,
      pm25_lag2, pm10_lag2, so2_lag2,
      temp_lag2, wind_lag2, humidity_lag2, 
      starts_with("month_"),
      starts_with("dayofweek_")
  )

train_workday_before <- train %>%
    select(
      acc, no2_lag3, co_lag3, o3_lag3,
      pm25_lag3, pm10_lag3, so2_lag3,
      temp_lag3, wind_lag3, humidity_lag3, 
      starts_with("month_"),
      starts_with("dayofweek_")
  )

test_workday_before <- test %>%
    select(
      acc, no2_lag3, co_lag3, o3_lag3,
      pm25_lag3, pm10_lag3, so2_lag3,
      temp_lag3, wind_lag3, humidity_lag3, 
      starts_with("month_"),
      starts_with("dayofweek_")
  )
```

In the original paper, we use a non-parametric analysis of construction accidents was conducted solely based on $NO_2$ levels. There, we suggested that the level of subsidy the state could offer a contractor to close a site for a day due to high pollution levels. However, this cost-benefit analysis had some limitations. 

First, it focused only on $NO_2$ without incorporating other factors, which might not yield the best predictions. Additionally, the analysis used data from the working day itself, which didn't allow for worker optimization in a site that closes.

By using the prediction method, we now can compare the predictability of the model with actual day data and three time intervals before. If these models exhibit comparable performance and can accurately predict accidents, it could be a cost-effective method for closing sites on those days since contractors can prepare in advance and reallocate workers between sites.

Next, I arrange the covariates and the dependent variable matrices for each time interval:
```{r, message=FALSE, warning=FALSE}
covariates_real_time <- train_real_time %>% 
  select(
    -acc
  ) %>%
  as.matrix()

covariates_morning_time <- train_morning_time %>% 
  select(
    -acc
  ) %>%
  as.matrix()

covariates_night_before <- train_night_before %>% 
  select(
    -acc
  ) %>%
  as.matrix()

covariates_workday_before <- train_workday_before %>% 
  select(
    -acc
  ) %>%
  as.matrix()

dependent_real_time <- train_real_time %>% 
  select(
    acc
  ) %>%
  as.matrix()

dependent_morning_time <- train_morning_time %>% 
  select(
    acc
  ) %>%
  as.matrix()

dependent_night_before <- train_night_before %>% 
  select(
    acc
  ) %>%
  as.matrix()

dependent_workday_before <- train_workday_before %>% 
  select(
    acc
  ) %>%
  as.matrix()
```

Then I fit the ridge regression models:
```{r, message=FALSE, warning=FALSE}
fit_ridge_real_time <- glmnet(
   x = covariates_real_time,
   y = dependent_real_time,
   alpha = 0
 )

fit_ridge_morning_time <- glmnet(
   x = covariates_morning_time,
   y = dependent_morning_time,
   alpha = 0
 )

fit_ridge_night_before <- glmnet(
   x = covariates_night_before,
   y = dependent_night_before,
   alpha = 0
 )

fit_ridge_workday_before <- glmnet(
   x = covariates_workday_before,
   y = dependent_workday_before,
   alpha = 0
 )
```

Next, I perform cross-validation on the ridge regression models for each time interval:
```{r, message=FALSE, warning=FALSE}
cv_ridge_real_time <- cv.glmnet(x = covariates_real_time, y = dependent_real_time)
cv_ridge_morning_time <- cv.glmnet(x = covariates_morning_time, y = dependent_morning_time)
cv_ridge_night_before <- cv.glmnet(x = covariates_night_before, y = dependent_night_before)
cv_ridge_workday_before <- cv.glmnet(x = covariates_workday_before, y = dependent_workday_before)
```

Next I examine the selected coefficients from the ridge regression models for each time interval:
```{r, message=FALSE, warning=FALSE}
ridge_real_time <- coef(cv_ridge_real_time, s = "lambda.min")
ridge_real_time

ridge_morning_time <- coef(cv_ridge_morning_time, s = "lambda.min")
ridge_morning_time

ridge_night_before <- coef(cv_ridge_night_before, s = "lambda.min")
ridge_night_before

ridge_workday_before <- coef(cv_ridge_workday_before, s = "lambda.min")
ridge_workday_before
```

```{r, message=FALSE, warning=FALSE}
covariates_real_time_test <- test_real_time %>% 
  select(
    -acc
  ) %>%
  as.matrix()

covariates_morning_time_test <- test_morning_time %>% 
  select(
    -acc
  ) %>%
  as.matrix()

covariates_night_before_test <- test_night_before %>% 
  select(
    -acc
  ) %>%
  as.matrix()

covariates_workday_before_test <- test_workday_before %>% 
  select(
    -acc
  ) %>%
  as.matrix()
```

The four following chunks predicts accidents based on the ridge models and creates a confusion matrix for each one. Since construction accidents are a rare occurrence, we need to set accordingly the rule for which we decide the rule deciding the model predicts there exist a high chance for an accident. I decided here to put the threshold both as one and a half, and twice the average probability of an accident, saying if the model predicts a higher probability than those values for a construction accident, it classifies its prediction as prediction of an accident.

As we will see in the following results, we need to figure out the ideal, and acceptable, number of times of a site closure. If we decide to close all sites year-round, we will avoid all accidents, but what is the ideal number of sites closure to accidents avoidance? That's an important question which requires extensive discussions in the public policy sphere. I will try to supply comparison between rules and days of closure. We will see that as we go further in time, the prediction itself will be less accurate but also less costly in terms of days of closure.

```{r Real Time Prediction:, message=FALSE, warning=FALSE}
# Set the rule for classifying predictions
rule1.5 <- mean(data$acc) * 1.5
rule2 <- mean(data$acc) * 2

# Predict accidents using the ridge model
ridge_pred_real_time <- predict(
  object = cv_ridge_real_time,
  newx = covariates_real_time_test,
  s = c("lambda.min")
) %>% 
  bind_cols(test_real_time) %>%
  select(acc, lambda.min) %>% 
  rename(pred = lambda.min)

ridge_pred_decision_real_time1.5 <- ridge_pred_real_time %>%
  mutate(
    decision = if_else(pred < rule1.5, "0", "1"),
  ) 

ridge_pred_decision_real_time2 <- ridge_pred_real_time %>%
  mutate(
    decision = if_else(pred < rule2, "0", "1"),
  ) 

ridge_conf_mat_decision_real_time1.5 <-
  ridge_pred_decision_real_time1.5 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_real_time2 <-
  ridge_pred_decision_real_time2 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_real_time1.5
ridge_conf_mat_decision_real_time2
```

```{r, include=FALSE}
(29 / (29+121))
(21 / (21+129))

((((134811 + 29) / (5583))/3)*2)
((((49781 + 21) / (5583))/3)*2)
```

We can see here that with the 1.5 rule and the 2 rule, we can avoid in real time 19.3% and 14% of the accidents respectively. But this requires immediate closure of each site for 16.1 and 5.9 times a year on average (134,840 and 49,802 site closures in the test sample only respectively -- 3 years sample of 5,583), which doesn't allow labor-reallocation and will likely require government subsidy, which would not cut-even on average (with respect to the government expenses on accidents), as seen in the original paper.

```{r Morning Time:, message=FALSE, warning=FALSE}
ridge_pred_morning_time <- predict(
  object = cv_ridge_morning_time,
  newx = covariates_morning_time_test,
  s = c("lambda.min")
) %>% 
  bind_cols(test_morning_time) %>%
  select(acc, lambda.min) %>% 
  rename(pred = lambda.min)


ridge_pred_decision_morning_time1.5 <- ridge_pred_morning_time %>%
  mutate(
    decision = if_else(pred < rule1.5, "0", "1"),
  ) 

ridge_pred_decision_morning_time2 <- ridge_pred_morning_time %>%
  mutate(
    decision = if_else(pred < rule2, "0", "1"),
  ) 

ridge_conf_mat_decision_morning_time1.5 <-
  ridge_pred_decision_morning_time1.5 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_morning_time2 <-
  ridge_pred_decision_morning_time2 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_morning_time1.5
ridge_conf_mat_decision_morning_time2
```

```{r, include = FALSE}
(38 / (38+112))
(21 / (21+129))

((((155943 + 38) / (5583))/3)*2)
((((54449 + 21) / (5583))/3)*2)
```
We can see here that with the 1.5 rule and the 2 rule, we can avoid with the morning data 25.3% and 14% of the accidents respectively. This isn't a great fall-off in numbers of accidents from the real time data, and can allow for some labor re-allocation. In terms of closure of sites, we are talking of about 18.6 and 6.5 times a year on average (155,943 and 54,449 site closures in the test sample alone respectively -- 3 years sample of 5,583).

```{r Night Before:, message=FALSE, warning=FALSE}
ridge_pred_night_before <- predict(
  object = cv_ridge_night_before,
  newx = covariates_night_before_test,
  s = c("lambda.min")
) %>% 
  bind_cols(test_night_before) %>%
  select(acc, lambda.min) %>% 
  rename(pred = lambda.min)

ridge_pred_decision_night_before1.5 <- ridge_pred_night_before %>%
  mutate(
    decision = if_else(pred < rule1.5, "0", "1"),
  ) 

ridge_pred_decision_night_before2 <- ridge_pred_night_before %>%
  mutate(
    decision = if_else(pred < rule2, "0", "1"),
  ) 

ridge_conf_mat_decision_night_before1.5 <-
  ridge_pred_decision_night_before1.5 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_night_before2 <-
  ridge_pred_decision_night_before2 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_night_before1.5
ridge_conf_mat_decision_night_before2
```

```{r, include = FALSE}
(42 / (42+108))
(13 / (13+137))

((((147077 + 42) / (5583))/3)*2)
((((28213 + 13) / (5583))/3)*2)
```

We can see here that with the 1.5 rule and the 2 rule, we can avoid in with the night before data 28% and 8.7% of the accidents respectively. These are great numbers of accidents that we can avoid, and which can absolutely allow for labor re-allocation. In terms of closure of sites, we are talking of about 17.6 and 3.4 times a year on average (147,077 and 28,213 site closures in the test sample alone respectively -- 3 years sample of 5,583).

```{r, message=FALSE, warning=FALSE}
ridge_pred_workday_before <- predict(
  object = cv_ridge_workday_before,
  newx = covariates_workday_before_test,
  s = c("lambda.min")
) %>% 
  bind_cols(test_workday_before) %>%
  select(acc, lambda.min) %>% 
  rename(pred = lambda.min)

ridge_pred_decision_workday_before1.5 <- ridge_pred_workday_before %>%
  mutate(
    decision = if_else(pred < rule1.5, "0", "1"),
  ) 

ridge_pred_decision_workday_before2 <- ridge_pred_workday_before %>%
  mutate(
    decision = if_else(pred < rule2, "0", "1"),
  ) 

ridge_conf_mat_decision_workday_before1.5 <-
  ridge_pred_decision_workday_before1.5 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_workday_before2 <-
  ridge_pred_decision_workday_before2 %>%
  mutate(
    acc = as.factor(acc),
    decision = as.factor(decision)
  ) %>% 
  conf_mat(acc, decision)

ridge_conf_mat_decision_workday_before1.5
ridge_conf_mat_decision_workday_before2
```

```{r, include = FALSE}
(30 / (30+120))
(4 / (4+146))

((((137706 + 30) / (5583))/3)*2)
((((13660 + 4) / (5583))/3)*2)
```
In this last model, contracters have a full day in advance to prepare. We can see here that with the 1.5 rule and the 2 rule, it is possible to avoid with the night before data: 20% and 2.7% of the accidents respectively. These are still great numbers of accidents that can be avoided, and which can allow for labor re-allocation. In terms of closure of sites, we are talking of about 16.4 and 1.6 times a year on average (137,706 and 13,660 site closures in the test sample alone respectively -- 3 years sample of 5,583). These numbers are subjectively high and low, and should be taken all together into account in public policy.

```{r, message=FALSE, warning=FALSE}
# 2 Times the Average Accidents Rate:
data.frame(
  Time_Interval = c("Real Time", "Morning Time", "Night Before", "Workday Before"),
  Percent_of_Accidents_Avoided = c(0.193, 0.253, 0.28, 0.2),
  Number_of_Workdays_Cancled_on_Site = c(16.1, 18.6, 17.6, 16.4)
)

# 1.5 Times the Average Accidents Rate:
data.frame(
  Time_Interval = c("Real Time", "Morning Time", "Night Before", "Workday Before"),
  Percent_of_Accidents_Avoided = c(0.14, 0.14, 0.08, 0.02),
  Number_of_Workdays_Cancled_on_Site = c(5.9, 6.5, 3.4, 1.6)
)
```

Overall, the performance of the models varies across the different time intervals, highlighting the importance of considering temporal factors when predicting construction accidents. Although this is the case, the relatively good prediction of construction accidents show that it is possible to avoid accidents by relying on environmental factors is possible on a certain level. Generally, it appears that the real-time and morning time models perform slightly better than the night before and workday before models. This can be of great implications as this offers a cost-efficient solution of better labor allocation in days with high probability to accidents.

# 5. Conclusions:

In conclusion, this project aimed to extend and compliment the analysis presented in the paper "Heads Up: Does Pollution Cause Construction Accidents?" by Victor Lavy, Genia Rachkovsky, and myself. By leveraging machine learning methodologies, I delved deeper into the relationship between pollution and construction accidents, focusing on Nitrogen Dioxide ($NO_2$) as the primary pollutant of interest.

Throughout the project, I followed a systematic approach. I first replicated the data used in the original study, and conducted the original basic econometric analysis. The regression results consistently indicated a statistically significant effect of $NO_2$ on the probability of construction accidents, highlighting the importance of addressing pollution in workplace safety protocols.

To further support and extend the analysis, I employed machine learning techniques from the course "Machine Learning for Economists". The double lasso methodology allowed me to select relevant variables, including both current and lagged pollutants, which reinforced the importance of considering the transient effects of pollutants. Additionally, I developed prediction models using Ridge regularized regression, aiming to assess the likelihood of construction accidents based on various environmental factors available. The models showed promise in accurately predicting accidents, providing insights for effective safety management and resource allocation.

In summary, the findings of this project can potentialy contribute to better public-policy planning and understanding of the relationship between environmental factor and construction accidents. The project's combination of traditional econometric methods and machine learning approaches provided, in my eyes, a comprehensive analysis, validating the original results and offering additional insights. Policymakers can potentially utilize these findings to improve workplace safety measures, implement pollution control strategies, and aim to allocate labor more effectively.

### Reference
Lavy, Victor, Genia Rachkovski, and Omry Yoresh. Heads Up: Does Air Pollution Cause Workplace Accidents?. No. w30715. National Bureau of Economic Research, 2022.